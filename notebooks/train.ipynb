{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d381ff5",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090553d2",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b857ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set wandb directories BEFORE importing wandb\n",
    "os.environ[\"WANDB_DIR\"] = \"D:/peak2vec/wandb\"  # Change to drive with more space\n",
    "os.environ[\"WANDB_CACHE_DIR\"] = \"D:/peak2vec/wandb_cache\"\n",
    "os.environ[\"WANDB_DATA_DIR\"] = \"D:/peak2vec/wandb_data\"\n",
    "os.environ[\"TMPDIR\"] = \"D:/peak2vec/temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88ce4d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import scanpy as sc\n",
    "from src.peak2vec import PeakDataset, peak2vec_collate, prepare_adata, get_sampling_distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b360f4",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b7004f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.Table.MAX_ARTIFACT_ROWS = 300_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c963db04",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dc33b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_downsample(\n",
    "    df: pd.DataFrame,\n",
    "    col: str,\n",
    "    n_per_class: int,\n",
    "    random_state: int = 0,\n",
    "    shuffle: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    g = df.groupby(col, group_keys=False, observed=True)\n",
    "\n",
    "    # sample up to n_per_class per group (no replacement)\n",
    "    out = g.apply(lambda x: x.sample(n=min(len(x), n_per_class), random_state=random_state), include_groups=True)\n",
    "\n",
    "    if shuffle:\n",
    "        out = out.sample(frac=1, random_state=random_state)\n",
    "\n",
    "    return out.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd7ece3",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e00fbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 10006 × 260822\n",
       "    obs: 'cisTopic_nr_frag', 'cisTopic_log_nr_frag', 'cisTopic_nr_acc', 'cisTopic_log_nr_acc', 'sample_id', 'barcode_rank', 'total_fragments_count', 'log10_total_fragments_count', 'unique_fragments_count', 'log10_unique_fragments_count', 'total_fragments_in_peaks_count', 'log10_total_fragments_in_peaks_count', 'unique_fragments_in_peaks_count', 'log10_unique_fragments_in_peaks_count', 'fraction_of_fragments_in_peaks', 'duplication_count', 'duplication_ratio', 'nucleosome_signal', 'tss_enrichment', 'pdf_values_for_tss_enrichment', 'pdf_values_for_fraction_of_fragments_in_peaks', 'pdf_values_for_duplication_ratio', 'barcode', 'celltype', 'n_features_per_cell', 'total_fragment_counts'\n",
       "    var: 'Chromosome', 'Start', 'End', 'Width', 'cisTopic_nr_frag', 'cisTopic_log_nr_frag', 'cisTopic_nr_acc', 'cisTopic_log_nr_acc', 'n_cells_per_feature', 'mean_counts', 'pct_dropout_by_counts', 'total_counts'\n",
       "    layers: 'binary'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata = sc.read_h5ad(\"data/pbmc10k_atac_filtered.h5ad\")\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75ae8172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 10006 × 260822\n",
       "    obs: 'cisTopic_nr_frag', 'cisTopic_log_nr_frag', 'cisTopic_nr_acc', 'cisTopic_log_nr_acc', 'sample_id', 'barcode_rank', 'total_fragments_count', 'log10_total_fragments_count', 'unique_fragments_count', 'log10_unique_fragments_count', 'total_fragments_in_peaks_count', 'log10_total_fragments_in_peaks_count', 'unique_fragments_in_peaks_count', 'log10_unique_fragments_in_peaks_count', 'fraction_of_fragments_in_peaks', 'duplication_count', 'duplication_ratio', 'nucleosome_signal', 'tss_enrichment', 'pdf_values_for_tss_enrichment', 'pdf_values_for_fraction_of_fragments_in_peaks', 'pdf_values_for_duplication_ratio', 'barcode', 'celltype', 'n_features_per_cell', 'total_fragment_counts'\n",
       "    var: 'Chromosome', 'Start', 'End', 'Width', 'cisTopic_nr_frag', 'cisTopic_log_nr_frag', 'cisTopic_nr_acc', 'cisTopic_log_nr_acc', 'n_cells_per_feature', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'Center'\n",
       "    layers: 'binary'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_adata(adata)\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8a85b4",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b484a145",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bb60aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Peak2Vec(nn.Module):\n",
    "    \"\"\"Skip-gram model over peaks with negative sampling.\"\"\"\n",
    "    def __init__(self, n_peaks, embedding_dim=64, pos_weight=1.0, sparse=True):\n",
    "        super(Peak2Vec, self).__init__()\n",
    "        self.dim = embedding_dim\n",
    "        self.pos_weight = pos_weight\n",
    "        self.embedding = nn.Embedding(n_peaks, embedding_dim, sparse=sparse)\n",
    "        self.reset_params()\n",
    "    \n",
    "    def reset_params(self):\n",
    "        nn.init.uniform_(self.embedding.weight, -0.5 / self.dim, 0.5 / self.dim)\n",
    "\n",
    "    def forward(self, peaks, peak_pairs, negatives):\n",
    "        \"\"\"\n",
    "        Compute the Skip-gram with Negative Sampling loss.\n",
    "        peaks: (B,) LongTensor of peak indices\n",
    "        peak_pairs: (B,) LongTensor of positive context peak indices\n",
    "        negatives: (B, K) LongTensor of negative sample peak indices\n",
    "        \"\"\"\n",
    "        # Embeddings\n",
    "        peak_emb = self.embedding(peaks)               # (B, D)\n",
    "        pair_emb = self.embedding(peak_pairs)          # (B, D)\n",
    "        neg_emb = self.embedding(negatives)            # (B, K, D)\n",
    "\n",
    "        # Compute similarity scores\n",
    "        pos_score = torch.sum(peak_emb * pair_emb, dim=1)                   # (B)\n",
    "        neg_score = torch.bmm(neg_emb, peak_emb.unsqueeze(2)).squeeze(2)    # (B, K)\n",
    "\n",
    "        # Loss\n",
    "        poss_loss = F.softplus(-pos_score)         # -log(sigmoid(x)) = softplus(-x)\n",
    "        neg_loss = F.softplus(neg_score).sum(1)      # -log(1 - sigmoid(x)) = softplus(x)\n",
    "        loss = (self.pos_weight * poss_loss + neg_loss).mean()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            stats = {\n",
    "                \"pos_score_mean\": pos_score.mean().detach(),\n",
    "                \"neg_score_mean\": neg_score.mean().detach(),\n",
    "                \"pos_loss_mean\":  poss_loss.mean().detach(),\n",
    "                \"neg_loss_mean\":  neg_loss.mean().detach(),\n",
    "            }\n",
    "        return loss, stats\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_peak_embeddings(self, normalize=True):\n",
    "        embeddings = self.embedding.weight.detach().cpu()\n",
    "        if normalize:\n",
    "            embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        return embeddings\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def most_similar(self, peak_idx, topk=10):\n",
    "        embeddings = self.get_peak_embeddings(normalize=True)  # (N, D)\n",
    "        peak_emb = embeddings[peak_idx:peak_idx+1]             # (1, D)\n",
    "        similarities = (embeddings @ peak_emb.t()).squeeze(1)  # (N)\n",
    "        values, indices = torch.topk(similarities, topk + 1, embeddings.size(0), largest=True)\n",
    "        # drop self if in topk\n",
    "        if indices and indices[0] == peak_idx:\n",
    "            indices, values = indices[1:], values[1:]\n",
    "        return indices[:topk], values[:topk]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8b4739",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c3a892f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "seed = 4\n",
    "n_pairs = 20\n",
    "n_negative = 20\n",
    "samples_per_epoch = 20000\n",
    "batch_size = 512\n",
    "embedding_dim = 128\n",
    "trans_fraction=0.2\n",
    "cis_window=500000\n",
    "same_chr_negative_prob=0.5\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c60a4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, keep = get_sampling_distributions(adata, t=5e-7, power=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b443768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PeakDataset(\n",
    "    X=adata.X,\n",
    "    chr=adata.var[\"Chromosome\"].values,\n",
    "    centers=adata.var[\"Center\"].values,\n",
    "    neg_distribution=neg,\n",
    "    keep_distribution=keep,\n",
    "    samples_per_epoch=samples_per_epoch,\n",
    "    n_pairs=n_pairs,\n",
    "    n_negative=n_negative,\n",
    "    seed=seed,\n",
    "    trans_fraction=trans_fraction,\n",
    "    cis_window=cis_window,\n",
    "    same_chr_negative_prob=same_chr_negative_prob,\n",
    ")\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=peak2vec_collate,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a45441c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4951bd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512]),\n",
       " torch.Size([512]),\n",
       " torch.Size([512, 20]),\n",
       " defaultdict(int,\n",
       "             {'chr12': 31,\n",
       "              'chr2': 39,\n",
       "              'chr14': 12,\n",
       "              'chr5': 24,\n",
       "              'chr17': 32,\n",
       "              'chr1': 48,\n",
       "              'chr4': 16,\n",
       "              'chr3': 26,\n",
       "              'chr19': 45,\n",
       "              'chr22': 16,\n",
       "              'chr16': 19,\n",
       "              'chr6': 35,\n",
       "              'chr11': 30,\n",
       "              'chr10': 16,\n",
       "              'chr18': 5,\n",
       "              'chr8': 18,\n",
       "              'chr7': 22,\n",
       "              'chr20': 12,\n",
       "              'chr9': 29,\n",
       "              'chrX': 8,\n",
       "              'chr21': 6,\n",
       "              'chr15': 18,\n",
       "              'chr13': 5}),\n",
       " tensor([153976,  33045, 175417, 171519,  71453, 214272,  23239,  65453,  50967,\n",
       "         224073, 251459, 186153, 202136, 204147, 190148,  71024, 225572,  84389,\n",
       "         225439,  75257, 141176, 203246, 139675, 128623, 154168, 227520, 156306,\n",
       "         216574, 160885, 225418, 160174, 208095,  83971, 111800, 129660,  90076,\n",
       "         225253, 128114, 145635, 250114,  85837,  97529,  62118,  75177, 128041,\n",
       "         227709,  36762,  55171, 223278, 107377,  41127,  61564, 225458,  43637,\n",
       "          85802, 241512,  11213,  96476, 117109,  11255, 240072, 155892, 197183,\n",
       "         223278,  97584, 117120,  75473, 171889, 223381, 191163,  14101, 258018,\n",
       "         103722, 155929, 141571, 120620, 214782,  14093, 204079, 245877, 157164,\n",
       "          48889, 133174,  70053, 128036, 141127,  82106,  72801, 180057,  48580,\n",
       "         141037, 135067, 236200,  90567,  70160,  82724,  11188,  32124, 209611,\n",
       "          11078, 251307, 155957,  50599, 234050,  19239,  84448, 211140, 159091,\n",
       "         154510, 107342,  97321,  34188, 120900, 239384, 171239,  20964, 255292,\n",
       "         202280, 240768, 228661, 253285,  74587,  31034, 244483, 154852, 249322,\n",
       "          39187, 202868, 156008, 119843, 154645, 153159,  81770, 188561, 155855,\n",
       "         138952, 117684,   8975, 224361,  97420,  46513, 179969,  47910,  11224,\n",
       "         141585,  88348,  30132,  10007, 247249,  99941,  82505, 111550,  11272,\n",
       "          83729, 174846, 228874, 215387, 202728, 225148,  16609, 207003, 234042,\n",
       "          56302, 161986, 234533,   7611, 258030,  17173,  84344, 128162,  11609,\n",
       "          98094, 155737, 229689, 140612, 185137,  89001,  70931,  33284, 245307,\n",
       "         193777, 143514,  22343,  91430, 221264, 117099, 248973,  63903, 216340,\n",
       "         117045,  82858, 143210, 117096, 201986, 119012, 180930,  34236, 209845,\n",
       "         118135, 147549, 137904,  48714,   9980,  75260, 225441, 235715,  33979,\n",
       "          34048, 246905, 165526,  33232, 118488, 191673,  70433,  70234,  97443,\n",
       "          97056, 117600,  58513,   8745, 173876,  86515, 173166, 128894,  97678,\n",
       "         218995,  61875, 131866,  10988, 122227, 236023, 101477, 118290,  10413,\n",
       "          71486,  49889, 108233, 194906, 258018, 183393,  49146,  64708, 229507,\n",
       "          31791,  23208, 134619, 173897,  50141, 159793,  42749, 155058, 250940,\n",
       "         223096, 225553,  12707,  48552,  30943,  75261, 189612, 193136, 246029,\n",
       "         144442,  39731,  10146,  84316,  82747, 156776, 108582,  43059,  62294,\n",
       "         166335, 121528,  71752, 169009, 184844, 141303,  96737, 216283,  36630,\n",
       "         250749, 203634, 204208,  34187, 257680, 118453,  84437, 210491, 228105,\n",
       "         151703, 225162,  50553,  42335, 107321,  50281, 132926,  83814, 183720,\n",
       "          49426,  98474,  84351, 189757, 128438, 240128, 182081,  48695, 223451,\n",
       "         118173, 257357, 161834,  41972, 250072,  64429, 241007, 140436,  12484,\n",
       "          18740, 110807, 165156,  96507, 193910, 250737,  18703, 181000,  88966,\n",
       "         250398, 129699, 170728,  33942, 222900, 258062, 223491,  33419, 179480,\n",
       "         141556,   8652, 107452, 204415, 118401, 204788,  96206, 118009,  65742,\n",
       "         246154, 139743,  51632, 140676,  19248, 230400, 122507, 116860,  31931,\n",
       "         180637,  97059, 117085,  89156, 224071, 140898, 118103, 181179, 239904,\n",
       "         110468,  14039,  11069, 141726,  51688,  32048,  20678, 157282,  62271,\n",
       "          39742,  82876, 197740, 243715, 193744, 183603,  97588, 257377,  13195,\n",
       "          71704,  43859, 111610, 105902,   9030, 231240,  73892, 110311, 140105,\n",
       "         214661,  90633,  83708,  33323, 209930, 233804, 107447,  97689, 162359,\n",
       "          36100, 156138, 164925,  39859, 253098, 171163, 224537, 195600, 165256,\n",
       "         250920, 183377,  36041, 179984, 255043, 140947,  31796,  72052, 250974,\n",
       "          70944,  52538,  10117,  62279,  73115, 107384, 224112,  17652, 171212,\n",
       "         194903,   6629,  14269, 156001,   8617, 120548, 145889, 117405, 179948,\n",
       "          46428,  64548, 239108, 141188,  38555, 128789,  29379, 107778,  32288,\n",
       "         124525, 112672,  34050,  49551, 108308, 118931, 118339,  50616,  62280,\n",
       "         141540, 131210,  86126,  70839, 223367,  83489,   9273,   7349,  15280,\n",
       "         141032,  94658, 141438, 239868, 161674,  11236, 203082, 204667, 146728,\n",
       "          70562, 218763,  14264, 204812, 155990, 156015,  80269, 259186,  82772,\n",
       "          88873, 204671, 190110, 193916, 220844, 179872,  82768, 185276,  97733,\n",
       "         202517, 143560, 209519,  59167,  50596,  62091,  13211, 239275,   8908,\n",
       "         202616, 102123, 224053,  82921,  24574, 225224, 189892,  51370,  72364,\n",
       "          57646, 233375, 204685, 233339, 207413, 191357,  33319, 156021]),\n",
       " tensor([212635,  35999, 174931, 171523,  66858, 208203,   2787,  62288,  48738,\n",
       "         224074, 233418, 187425, 204783, 214219, 188183,  75231, 228088,  82762,\n",
       "         233418,   9580, 145771, 211963, 257753, 133458, 156306, 224515, 154168,\n",
       "         215699, 154804,   8455, 155584, 202242,  90915, 107797, 135149,  88998,\n",
       "         225817,  71697, 141565, 248088, 240057, 153222,   8013,  72637, 131516,\n",
       "         108682,  36761,  49191, 223269, 112696,  43443, 155239, 224963,  43328,\n",
       "          83581, 239366, 156855,  97739,  10925,  11361, 243760, 156017, 187050,\n",
       "         230802, 223672, 117943,  70536, 171814, 223382, 191158,  25646, 260636,\n",
       "          98081, 157540, 140541, 117815, 204791,  11345, 204076, 245876, 257655,\n",
       "          50710, 129255,  70313, 128045, 141140,  83109,  76954, 184887,  48569,\n",
       "         148386, 129602, 235326,  82956, 224324,  78303,  11190,  42317, 202979,\n",
       "          24124, 251303, 155523, 125464, 223485,  10804,  84449, 204580,  75500,\n",
       "         155867, 108281,  97322,  29430, 108523, 239395,  89502,   9579, 250386,\n",
       "         204593, 243865, 252864, 252104,  70884,  91266, 239314, 154866, 249918,\n",
       "          37749, 209587, 154658, 121654,  52549, 155915,  84523, 180294, 135696,\n",
       "         190194, 120325,   8971, 227398, 103547,  47948, 179963,  54562,  11223,\n",
       "         143197,  84069,  32216,  19087, 245962,  97070, 108677, 107761,  33305,\n",
       "         117089, 171872,  98643, 258039, 202757, 225136,  16972, 214033, 225298,\n",
       "          55354, 155586, 224661, 250858,  11308, 108426,  88451,    993,  19360,\n",
       "          98093, 155748, 257137, 147790, 181009,  88677,  70934, 257879, 246498,\n",
       "         189567, 141210, 188533,  89036, 233306, 117096, 248053,  63584, 217015,\n",
       "         118409,  82860, 143217, 122396, 202677, 121869, 184173,  37163, 190303,\n",
       "         118133, 135707, 139779,  48715,  83852, 157813, 224801, 226430,  32070,\n",
       "          32837, 113929, 167579,  33229, 119706, 190400,  71467, 225023,  99623,\n",
       "         103044, 118533,  61305,  12970, 170927,  89634, 173158, 209882,  96963,\n",
       "          72085,  50713, 127620,   7663, 121722, 175918,  97075, 203594,  14918,\n",
       "          70613,  49890, 109672, 160172, 259237, 179940,  49132,  62352, 223462,\n",
       "          26118,  62298, 129609, 176096,  52697, 193904,  39501, 160100, 254383,\n",
       "         232836, 224753,  21108,  56942,  32706,  75260, 189629,  73409, 246557,\n",
       "         140778,  16609,   6318,  84525,  82734, 155876, 107536,  32466, 118459,\n",
       "         164856, 120565,  71746, 173020, 179832, 141309, 101431, 204162,  33494,\n",
       "         250975, 209717, 211968,  29662, 258048, 121749,  88939, 171895, 225143,\n",
       "         155963, 109515,  48767,  32504,  75142,  53550, 130241,   5727, 205328,\n",
       "          50609,  96609, 210652, 194424, 129587, 231842, 180708,  55348, 226324,\n",
       "         208215, 258159, 159037,  32147, 250073,  61985, 237380, 140451,   8254,\n",
       "           7844, 110818, 224387,  96489, 194100, 250975,   7659, 180895,  84463,\n",
       "         250949, 127556, 257776,  43253, 225496, 258004, 223443,  36527,  55128,\n",
       "         146853,  11079, 203679, 207276, 132893,  57238,  94940, 122452,  59079,\n",
       "         245699, 148534,  51609, 142848,  11108, 243050, 225574, 121644,  26298,\n",
       "         180636,  97756, 115627,  82485, 227022, 143047, 209461, 180902, 241805,\n",
       "         204778,   4824,   8565,  63465,  49368,  32047,  16004, 156023,  61657,\n",
       "          31806,  84402, 129901, 239292, 186279, 179883, 250974, 258183,  13193,\n",
       "          70084,  31812, 108239, 238966, 223791, 225395,  70163, 165299, 141438,\n",
       "         203083,  83971,  84486,  33324, 202055, 224593, 107448,  97048, 160988,\n",
       "          40600, 160968, 164931,  37434, 253884, 171162, 235428, 188764, 155313,\n",
       "         253398, 180900,  33075, 224496, 249766, 140955,  31791,  70335, 250975,\n",
       "          72848,  50043,  11227,  60558,  71148, 107395, 225369,  20126, 230399,\n",
       "         189572,  21033, 148764, 154322,   8636, 117146, 223553, 124222, 185357,\n",
       "          50514,  60987, 129151, 143486, 243018, 128788,  33347, 109142,  34004,\n",
       "         124483, 108062,  32872,  57151, 108309, 132998, 118308,  49502,  66037,\n",
       "         149358, 129276,  83812,  71512, 223364,  83499,  33989,   7358,  16364,\n",
       "         141516,  30643,  97518, 243522, 154266, 239099, 158698, 203082, 144392,\n",
       "          71480,  34089,   9801, 203460,  94863,   9415,  82475, 259922, 190236,\n",
       "         133101, 205942, 225245, 192136, 225008, 181225,  82762, 185183,  97995,\n",
       "         245861, 143572, 205243, 206237,  49279,  65703,   8971, 136077, 129046,\n",
       "         202593,  95939, 224052,  84408,   8010, 231604, 186889,  54114,  75827,\n",
       "          47868, 230344, 108594, 235883, 207421, 193721,  71547, 159663]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peaks, peak_pairs, negatives = next(iterator)\n",
    "peaks.size(), peak_pairs.size(), negatives.size(), dataset.counter, peaks, peak_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d8ca32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse = False\n",
    "pos_weight = 10.0\n",
    "lr = 2e-3\n",
    "model = Peak2Vec(adata.n_vars, embedding_dim=embedding_dim, pos_weight=pos_weight, sparse=sparse).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88dec4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'seed': seed,\n",
    "        'n_pairs': n_pairs,\n",
    "        'n_negative': n_negative,\n",
    "        'samples_per_epoch': samples_per_epoch,\n",
    "        'batch_size': batch_size,\n",
    "        'embedding_dim': embedding_dim,\n",
    "        'trans_fraction': trans_fraction,\n",
    "        'cis_window': cis_window,\n",
    "        'same_chr_negative_prob': same_chr_negative_prob,\n",
    "        'device': device,\n",
    "        'embedding_dim': embedding_dim,\n",
    "        'pos_weight': pos_weight,\n",
    "        'sparse': sparse,\n",
    "        'optimizer': str(optimizer),\n",
    "        'learning_rate': lr,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80817c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\claptar\\AppData\\Local\\Temp\\ipykernel_14780\\2239658404.py:11: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = g.apply(lambda x: x.sample(n=min(len(x), n_per_class), random_state=random_state), include_groups=True)\n"
     ]
    }
   ],
   "source": [
    "downsampled_peaks = balanced_downsample(adata.var.Chromosome.to_frame().reset_index(), 'Chromosome', 1200)\n",
    "downsampled_idx = downsampled_peaks.index.tolist()\n",
    "downsampled_rows = downsampled_peaks['RegionIDs'].tolist()\n",
    "downsampled_chr = downsampled_peaks['Chromosome'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22df9475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>data\\wandb\\wandb\\run-20251229_170315-peak2vec-8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/claptar/peak2vec/runs/peak2vec-8' target=\"_blank\">peak2vec-test-run8</a></strong> to <a href='https://wandb.ai/claptar/peak2vec' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/claptar/peak2vec' target=\"_blank\">https://wandb.ai/claptar/peak2vec</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/claptar/peak2vec/runs/peak2vec-8' target=\"_blank\">https://wandb.ai/claptar/peak2vec/runs/peak2vec-8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\claptar\\AppData\\Local\\Temp\\ipykernel_14780\\1365935682.py\", line 63, in <module>\n",
      "    run.log({\"peak_embedding\": table})\n",
      "    ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\peak2vec\\.venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 400, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"c:\\projects\\peak2vec\\.venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 458, in wrapper_fn\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"c:\\projects\\peak2vec\\.venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 445, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"c:\\projects\\peak2vec\\.venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 2034, in log\n",
      "    self._log(data=data, step=step, commit=commit)\n",
      "    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\peak2vec\\.venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1745, in _log\n",
      "    self._partial_history_callback(data, step, commit)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\peak2vec\\.venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 400, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"c:\\projects\\peak2vec\\.venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1573, in _partial_history_callback\n",
      "    self._backend.interface.publish_partial_history(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self,\n",
      "        ^^^^^\n",
      "    ...<4 lines>...\n",
      "        publish_step=not_using_tensorboard,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\projects\\peak2vec\\.venv\\Lib\\site-packages\\wandb\\sdk\\interface\\interface.py\", line 689, in publish_partial_history\n",
      "    data = history_dict_to_json(run, data, step=user_step, ignore_copy_err=True)\n",
      "  File \"c:\\projects\\peak2vec\\.venv\\Lib\\site-packages\\wandb\\sdk\\data_types\\utils.py\", line 54, in history_dict_to_json\n",
      "    payload[key] = val_to_json(\n",
      "                   ~~~~~~~~~~~^\n",
      "        run, key, val, namespace=step, ignore_copy_err=ignore_copy_err\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\projects\\peak2vec\\.venv\\Lib\\site-packages\\wandb\\sdk\\data_types\\utils.py\", line 158, in val_to_json\n",
      "    val.bind_to_run(run, key, namespace)\n",
      "    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\projects\\peak2vec\\.venv\\Lib\\site-packages\\wandb\\sdk\\data_types\\table.py\", line 609, in bind_to_run\n",
      "    with codecs.open(tmp_path, \"w\", encoding=\"utf-8\") as fp:\n",
      "         ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen codecs>\", line 921, in open\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\claptar\\\\AppData\\\\Local\\\\Temp\\\\tmp0o5gjlo7wandb-media\\\\dob7vzem.table.json'\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch_time_sec</td><td>▅▃▅▄▅▆▆█▆▅▄▄▄▆▅▅▅█▆▆▇▅▄▆▆▅▅▆▆▆▅▅▆▅▆▁▃▁▃▄</td></tr><tr><td>loss</td><td>██▇▆▆▅▅▅▅▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>neg_loss</td><td>██████████▇▇▇▇▆▅▅▅▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>neg_score</td><td>█████████████▇▇▇▇▇▇▆▆▆▅▅▅▄▄▃▃▃▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>pos_score</td><td>▁▁▁▂▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>poss_loss</td><td>█▇▆▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>samples_per_second</td><td>▃▃▃▄▃▄▁▂▂▃▄▄▄▂▃▃▁▁▃▂▄▃▄▂▃▂▃▃▂▂▄▄▃▃▂▅█▆█▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch_time_sec</td><td>103.27192</td></tr><tr><td>loss</td><td>7.07379</td></tr><tr><td>neg_loss</td><td>7.00946</td></tr><tr><td>neg_score</td><td>-1.55292</td></tr><tr><td>pos_score</td><td>5.21119</td></tr><tr><td>poss_loss</td><td>0.00643</td></tr><tr><td>samples_per_second</td><td>193.66348</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">peak2vec-test-run8</strong> at: <a href='https://wandb.ai/claptar/peak2vec/runs/peak2vec-8' target=\"_blank\">https://wandb.ai/claptar/peak2vec/runs/peak2vec-8</a><br> View project at: <a href='https://wandb.ai/claptar/peak2vec' target=\"_blank\">https://wandb.ai/claptar/peak2vec</a><br>Synced 4 W&B file(s), 15 media file(s), 62 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>data\\wandb\\wandb\\run-20251229_170315-peak2vec-8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\claptar\\\\AppData\\\\Local\\\\Temp\\\\tmp0o5gjlo7wandb-media\\\\dob7vzem.table.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     61\u001b[39m emb_df[\u001b[33m\"\u001b[39m\u001b[33mchromosome\u001b[39m\u001b[33m\"\u001b[39m] = downsampled_chr\n\u001b[32m     62\u001b[39m table = wandb.Table(dataframe=emb_df)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpeak_embedding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Save unnormalized embeddings as a wandb Artifact\u001b[39;00m\n\u001b[32m     66\u001b[39m artifact = wandb.Artifact(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_peak_embeddings_epoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\peak2vec\\.venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:400\u001b[39m, in \u001b[36m_log_to_run.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    397\u001b[39m     run_id = \u001b[38;5;28mself\u001b[39m._attach_id\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m wb_logging.log_to_run(run_id):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\peak2vec\\.venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:458\u001b[39m, in \u001b[36m_raise_if_finished.<locals>.wrapper_fn\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    455\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m: Run, *args: _P.args, **kwargs: _P.kwargs) -> _T:\n\u001b[32m    457\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_finished\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m     message = (\n\u001b[32m    461\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRun (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) is finished. The call to\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    462\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` will be ignored.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    463\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Please make sure that you are using an active run.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    464\u001b[39m     )\n\u001b[32m    466\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UsageError(message)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\peak2vec\\.venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:445\u001b[39m, in \u001b[36m_attach.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    443\u001b[39m         _is_attaching = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\peak2vec\\.venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:2034\u001b[39m, in \u001b[36mRun.log\u001b[39m\u001b[34m(self, data, step, commit)\u001b[39m\n\u001b[32m   2027\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._settings._shared \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2028\u001b[39m     wandb.termwarn(\n\u001b[32m   2029\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn shared mode, the use of `wandb.log` with the step argument is not supported \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2030\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mand will be ignored. Please refer to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl_registry.url(\u001b[33m'\u001b[39m\u001b[33mdefine-metric\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2031\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mon how to customize your x-axis.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2032\u001b[39m         repeat=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2033\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2034\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\peak2vec\\.venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:1745\u001b[39m, in \u001b[36mRun._log\u001b[39m\u001b[34m(self, data, step, commit)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m data.keys()):\n\u001b[32m   1743\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mKey values passed to `wandb.log` must be strings.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1745\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_partial_history_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1748\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.getpid() != \u001b[38;5;28mself\u001b[39m._init_pid \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_attached:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\peak2vec\\.venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:400\u001b[39m, in \u001b[36m_log_to_run.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    397\u001b[39m     run_id = \u001b[38;5;28mself\u001b[39m._attach_id\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m wb_logging.log_to_run(run_id):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\peak2vec\\.venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:1573\u001b[39m, in \u001b[36mRun._partial_history_callback\u001b[39m\u001b[34m(self, data, step, commit)\u001b[39m\n\u001b[32m   1570\u001b[39m data = \u001b[38;5;28mself\u001b[39m._serialize_custom_charts(data)\n\u001b[32m   1572\u001b[39m not_using_tensorboard = \u001b[38;5;28mlen\u001b[39m(wandb.patched[\u001b[33m\"\u001b[39m\u001b[33mtensorboard\u001b[39m\u001b[33m\"\u001b[39m]) == \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1573\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish_partial_history\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1574\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1575\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1576\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_step\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1577\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1578\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflush\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1579\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpublish_step\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnot_using_tensorboard\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1580\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\peak2vec\\.venv\\Lib\\site-packages\\wandb\\sdk\\interface\\interface.py:689\u001b[39m, in \u001b[36mInterfaceBase.publish_partial_history\u001b[39m\u001b[34m(self, run, data, user_step, step, flush, publish_step)\u001b[39m\n\u001b[32m    680\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish_partial_history\u001b[39m(\n\u001b[32m    681\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    682\u001b[39m     run: Run,\n\u001b[32m   (...)\u001b[39m\u001b[32m    687\u001b[39m     publish_step: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    688\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m689\u001b[39m     data = \u001b[43mhistory_dict_to_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_copy_err\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    690\u001b[39m     data.pop(\u001b[33m\"\u001b[39m\u001b[33m_step\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    692\u001b[39m     \u001b[38;5;66;03m# add timestamp to the history request, if not already present\u001b[39;00m\n\u001b[32m    693\u001b[39m     \u001b[38;5;66;03m# the timestamp might come from the tensorboard log logic\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\peak2vec\\.venv\\Lib\\site-packages\\wandb\\sdk\\data_types\\utils.py:54\u001b[39m, in \u001b[36mhistory_dict_to_json\u001b[39m\u001b[34m(run, payload, step, ignore_copy_err)\u001b[39m\n\u001b[32m     50\u001b[39m         payload[key] = history_dict_to_json(\n\u001b[32m     51\u001b[39m             run, val, step=step, ignore_copy_err=ignore_copy_err\n\u001b[32m     52\u001b[39m         )\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m         payload[key] = \u001b[43mval_to_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_copy_err\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_copy_err\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m payload\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\peak2vec\\.venv\\Lib\\site-packages\\wandb\\sdk\\data_types\\utils.py:158\u001b[39m, in \u001b[36mval_to_json\u001b[39m\u001b[34m(run, key, val, namespace, ignore_copy_err)\u001b[39m\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Partitioned tables and joined tables do not support being bound to runs.\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[32m    155\u001b[39m         \u001b[38;5;28mhasattr\u001b[39m(val, \u001b[33m\"\u001b[39m\u001b[33m_log_type\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    156\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m val._log_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mpartitioned-table\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mjoined-table\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    157\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m         \u001b[43mval\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind_to_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m res = val.to_json(run)\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, wandb.Table) \u001b[38;5;129;01mand\u001b[39;00m val.log_mode == \u001b[33m\"\u001b[39m\u001b[33mINCREMENTAL\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# Set the _last_logged_idx AFTER the Table has been logged and\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# bound to the run.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\peak2vec\\.venv\\Lib\\site-packages\\wandb\\sdk\\data_types\\table.py:609\u001b[39m, in \u001b[36mTable.bind_to_run\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    607\u001b[39m tmp_path = os.path.join(MEDIA_TMP.name, runid.generate_id() + \u001b[33m\"\u001b[39m\u001b[33m.table.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    608\u001b[39m data = _numpy_arrays_to_lists(data)\n\u001b[32m--> \u001b[39m\u001b[32m609\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[32m    610\u001b[39m     util.json_dump_safer(data, fp)\n\u001b[32m    611\u001b[39m \u001b[38;5;28mself\u001b[39m._set_file(tmp_path, is_tmp=\u001b[38;5;28;01mTrue\u001b[39;00m, extension=\u001b[33m\"\u001b[39m\u001b[33m.table.json\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:921\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(filename, mode, encoding, errors, buffering)\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\claptar\\\\AppData\\\\Local\\\\Temp\\\\tmp0o5gjlo7wandb-media\\\\dob7vzem.table.json'"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "with wandb.init(\n",
    "    entity=\"claptar\",\n",
    "    project=\"peak2vec\",\n",
    "    dir=\"./data/wandb\",\n",
    "    id=\"peak2vec-8\",\n",
    "    name=\"peak2vec-test-run8\",\n",
    "    notes=\"Testing Peak2Vec with sparse embeddings and modified loss\",\n",
    "    tags=[\"test\", \"first_run\"],\n",
    "    config=config,\n",
    "    mode=\"online\",\n",
    "    resume=\"allow\",\n",
    ") as run:\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "\n",
    "        running = 0.0\n",
    "        running_pos = 0.0\n",
    "        running_neg = 0.0\n",
    "        pos_score = 0.0\n",
    "        neg_score = 0.0\n",
    "        #dataset.set_epoch(epoch)\n",
    "        for step, (peaks, peak_pairs, negatives) in enumerate(loader, 1):\n",
    "            peaks = peaks.to(device, non_blocking=True)\n",
    "            peak_pairs = peak_pairs.to(device, non_blocking=True)\n",
    "            negatives = negatives.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss, stats = model(peaks, peak_pairs, negatives)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            running += float(loss.detach().cpu())\n",
    "            running_pos += float(stats[\"pos_loss_mean\"].cpu())\n",
    "            running_neg += float(stats[\"neg_loss_mean\"].cpu())\n",
    "            pos_score += float(stats[\"pos_score_mean\"].cpu())\n",
    "            neg_score += float(stats[\"neg_score_mean\"].cpu())\n",
    "\n",
    "            #print(f\"Epoch {epoch} | Step {step:03d} | Loss: {running / step:.4f} | Pos Loss: {stats['pos_loss_mean']:.4f} | Neg Loss: {stats['neg_loss_mean']:.4f} | Pos Score: {stats['pos_score_mean']:.4f} | Neg Score: {stats['neg_score_mean']:.4f}\")\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        # Log epoch stats\n",
    "        run.log({\n",
    "            \"loss\": running / step,\n",
    "            \"poss_loss\": running_pos / step,\n",
    "            \"neg_loss\": running_neg / step,\n",
    "            \"pos_score\": pos_score / step,\n",
    "            \"neg_score\": neg_score / step,\n",
    "            \"epoch_time_sec\": epoch_time,\n",
    "            \"samples_per_second\": samples_per_epoch / epoch_time,\n",
    "        })\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == num_epochs:\n",
    "            embedding = model.get_peak_embeddings(normalize=False).numpy()\n",
    "            embedding_norm = model.get_peak_embeddings(normalize=True).numpy()\n",
    "            # Save normalized embeddings as a wandb Table\n",
    "            emb_df = pd.DataFrame(embedding_norm[downsampled_idx], index=downsampled_rows, columns=[f\"dim_{i}\" for i in range(embedding.shape[1])])\n",
    "            emb_df[\"chromosome\"] = downsampled_chr\n",
    "            table = wandb.Table(dataframe=emb_df)\n",
    "            run.log({\"peak_embedding\": table})\n",
    "\n",
    "            # Save unnormalized embeddings as a wandb Artifact\n",
    "            artifact = wandb.Artifact(f\"{run.name}_peak_embeddings_epoch_{epoch}\", type=\"dataset\")\n",
    "            np.save(f\"data/{run.id}_peak_embeddings_epoch_{epoch}.npy\", embedding)\n",
    "            artifact.add_file(f\"data/{run.id}_peak_embeddings_epoch_{epoch}.npy\", skip_cache=True, policy=\"mutable\")\n",
    "            run.log_artifact(artifact)\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e00b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peak2vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
